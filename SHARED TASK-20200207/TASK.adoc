= SCRAP TWITTER API and FINANCIAL SENTIMENT ANALYSIS

Use case: +
we want to get a specific bunch of tweets from their given IDs. We want these twwets specifically because these IDS are provided with a sentiment analysis labels,
so we want to recover the text associated to these sentiments, then predict sentiment associated to BAIDU's index on twitter


== Create dev environment on Twitter
create a dev account on https://developer.twitter.com[twitter dev] +
create an App with customer and access tokens

== Create local environement
Create a virtual environement `nlpbase` with python3.6 +
Install tweepy. The syntax for declaring API in tweepy is +

**auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)** +
**auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)**

== Deliverable 1

Create a scraper that will

 - parse the Twitter API and recover the tweet text only from the ID given in the `BIDU.txt` file, then merge these texts with sentiment labels exactly
 - for this exercise we will assume that you will have to send your request by batches as the API won't accept a request of a 10.000 and we have network issues, so we want to write sequentially on the disk each time we get a batch of tweets

the deliverable is a simple cmd line tool

== Deliverable two

Prepare the training text:
 - suppress the urls
 - supress the mentions
 - normalize text in a way that will be simple to reproduce

== Deliverable three

Write a script predicting sentiment on BAIDU's index from the scrapped data

