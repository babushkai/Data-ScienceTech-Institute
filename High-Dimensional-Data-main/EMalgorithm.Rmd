---
title: "R Notebook"
output: html_notebook
---


## The GMM and the EM algorithm

The EM algorithm is implemented for R in the 'mclust'

```{r}
#install.packages("mclust")
library(mclust)
```

The help of the package is available:

```{r}
?Mclust
```

```{r}
data(swiss)
out=Mclust(swiss)
plot(swiss, col=out$classification)
```
> GMM model seems working better on large and small clusters ex)education for catholic
> Exercise: have a look at what is inside 'out' and to relate it with the GMM.

We can observe posterior probabilities are listed from high to low
each mean of variable in each cluster is shown here.
```{r}
out$parameters
```

Evaluation of the quality of model used to compare several models
```{r}
out$loglik
```

> Exercise: Compare the GMM clustering with the ones obtained with kmeans and hclust

## K-means
```{r}
out5 = kmeans(swiss, centers=3, nstart=10)
plot(swiss, col=out5$cluster, pch=19)
```

## Hierarchical Clustering
```{r}

library(class)
?hclust
D = dist(swiss)

par(mfrow=c(2,2))
hc1 = hclust(D, method="complete"); plot(hc1)
hc2 = hclust(D, method="single"); plot(hc2)
hc3 = hclust(D, method="centroid"); plot(hc3)
hc4 = hclust(D, method="ward.D2");plot(hc4)
```


```{r}

# Comparison with kmeans and hclust 
par(mfrow=c(1,3))
hc = hclust(D, method="single"); out.hc=cutree(hc, 4)
plot(swiss[,c(5,2)], col=out.hc, pch=19, cex=3, main="HC(single)")
out.km = kmeans(swiss, centers=3, nstart=10)
plot(swiss[,c(5,2)], col=out.km$cluster, pch=19, cex=3, main="kmeans")
plot(swiss[,c(5,2)], col=out$class, phc=19, cex=3, main="GMM+EM algorithm")
```

